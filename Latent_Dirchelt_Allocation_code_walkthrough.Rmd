---
title: "LDA Wiki Text Processing Notebook"
output:
  pdf_document: default
  html_notebook: default
---



Lets read in our necesary libraries, you may need to use install.packages() to install uninstalled packages before reading them in
```{r}
library(textreg)
library(stmBrowser)
library(corpustools)
library(dplyr)
library(tidytext)
library(quanteda)
library(devtools)
library(tm)
library(SnowballC)
library(topicmodels)
library(slam)
library(LDAvis)
library(stm)
library(Rtsne)
library(servr)
library(data.table)
library(readtext)
```


First create a corpus from your directory where are all the wiki text files are stored, and then inspect the first doc to make sure your text files have read in correctly
```{r}
original_txt_files = c("/Users/t_naraa/Dropbox/ldawiki_try5")
docs_dir = Corpus(DirSource(original_txt_files, pattern = 'txt'))
writeLines(as.character(docs_dir[1]))
```

Then go through some text pre-processing steps 
  1) Define custom stopwords and delete them
  2) Strip whitespace

```{r}
# Define custom stopwords and delete them
myStopwords = c(" f", "a","r","t","c","v","version","png","title","attachment","attachments","section","description", "document",
                "modificationdate", "date","href", 
                "able","about","across","after","all","almost","also","am","among","an",
                "and","any","are","as","at","be","because","been","but","by","can","cannot","could","dear",'did','do','does',
                'either','else','ever','every','for','from','get','got','had','has','have','he','her','hers','him','his','how',
                'however','i','if','in','into','is','it','its','just','least','let','like','likely','may','me','might','most',
                'must','my','neither','no','nor','not','of','off','often','on','only','or','other','our','own','rather','said',
                'say','says','she','should','since','so','some','than','that','the','their','them','then','there','these','they',
                'this','tis','to','too','twas','us','wants','was','we','were','what','when','where','which','while','who','whom',
                'why','will','with','would','yet','you','your','subject','sent','manager', 'owner', 'stakeholder', 'use' ,
                'technical','business',"problem",'statement', 'date', 'opened','question', 'decision', 'action','items', 'meeting',
                'notes',"xls",'dbo','dbname','asp','dmg','introduction','project','xyz','todateyyyymmdd', 'yyyymmdd','paragraph',
                'unzip','xdd','documentdocx','docx',"pdf","vpdf","ppt")
docs <- tm_map(docs_dir, removeWords, myStopwords)
docs <- tm_map(docs, stripWhitespace)


```

I process the text even more by removing words that are not frequent and too frequent. The global argument in control below means terms that appear in less documents than the lower bound  
or in more documents than the upper bound are deleted. We set it to (3,40) and then create a dtm (document term matrix). The document term matrix is a way to store data on the frequency of words within documents in a concise format. It is a 3-dimensional matrix where the columsn are every unique word in the corpus, and the rows are each document, and in each row there is a frequency count of how many times that unique vocab word appears in the doc. Look at the output of the below code chunnk to understand this better 

```{r}
limits = c(3,40)
dtm <- DocumentTermMatrix(docs, control = list(bounds= list(global = limits)))
inspect(DocumentTermMatrix(docs,control = list(bounds= list(global = limits))))

```



Next, we measure the term frequency-inverse document frequency scores(tf-idf) and examine distribution, which allows us to decide threshold to omit terms which have low frequency as well as those occurring in many documents (ie low tfidf scores). Tf-idf is a measure of how important each word is in a corpus that takes into account frequency and exclusivity. For a better explanation see here: 

```{r}
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean)*log2(nDocs(dtm)/col_sums(dtm > 0))
quantile(term_tfidf, c(0.05 * (1:20))) 
```

Let's only keep words with tfidf scores over .1 and see how many words that leaves us with

```{r}
dtm_tfidf = dtm[,term_tfidf>=.1]
dtm_tfidf = dtm_tfidf[row_sums(dtm_tfidf)>0,]
length(Terms(dtm_tfidf))
```

Next we want to convert this 8561 term dtm into a named text vector so it can work directly with stm package. This may seem like a roundabout way of storing our data but it's the only way that properly conserves document ids for later tagging purposes thats compatible with the stm package. Ways that I've tried to include the document id's as metadata and have failed at include:
  1) Trying to attach the metadata to the dtm, converting the dtm to a dfm (from the quanteda package) and then directly using the dfm with the stm package for estimating the model
  2) converting dtmlist into a tm corpus, writing that corpus to another text directory and then reading in that text directory. 


```{r}     
#Converting dtm into text vector
dtm2list <- apply(dtm_tfidf, 1, function(x) {
    paste(rep(names(x), x), collapse=" ")
})

```

 
And now we can use the stm package directly with the new directory, with the textProcessor and prepDocuments functions

```{r}

processed_3_40_stemmed = textProcessor(dtm2list, meta = as.data.frame(names(dtm2list)), removestopwords = F,  stem = T, wordLengths = c(3,20), verbose = F, onlytxtfiles = T)
out_3_40_stemmed = prepDocuments(processed_3_40_stemmed$documents, processed_3_40_stemmed$vocab, processed_3_40_stemmed$meta)


#saveRDS(out_3_40_stemmed, file = "out_3_40_stemmed.rds")

```


So now we have the out_* object for use directly with stm models. First thing we want to do is to find an optimal topic number (K) for the corpus. The below code chunk is a searching function that will take a long time to run but worth it to see how different topics numbers fare in terms of exclusivity, semantic coherence, residuals, and lower bound which will be plotted at the end.What we really care about is the semantic coherence plot. That tellls us approx how self contained and stable topics are, and generally higher semantic coherence means easier to identify topics. So we choose the K with the highest semantic coherence. 

```{r}
# Will take a long time to run (atleast 2 hours)
ksearch_lim_3_40_stemmed = searchK(out_3_40_stemmed$documents, out_3_40_stemmed$vocab, K = c(5:100))

# Will plot semantic coherence and exclusivity scores for each of the K's                         
plot(ksearch_lim_3_40_stemmed)
max_semcoh_index = which.max(ksearch_lim_3_40_stemmed$results$semcoh)
optimK = ksearch_lim_3_40_stemmed$results$K[[max_semcoh_index]]

```

We then use the selectModel() function in the stm package to run multipe iterarions of K topic models using the optimalK. This function will only keep the top 10% of runs and plot the top models based on semantic coherence and exclusivity. You can look at the graph and make your own determination for which model to use. But as a general rule, choosing model with the highest avg semantic coherence score is a good way to go. 


```{r}
#SelectModel object for choosing best 32 topic stemmed model
stemmed32_lim3_40_models = selectModel(out_3_40_stemmed$documents, out_3_40_stemmed$vocab, K =optimK)

# Use this to plot the many models based on exclusivity and semantic coherence
plotModels(stemmed32_lim3_40_models)

# Can choose model with highest avg semcoh score using this
semcoh_means=(lapply(stemmed32_lim3_40_models$semcoh,mean))
max_index = which.max(unlist(semcoh_means))-1
opt_model32 = stemmed32_lim3_40_models$runout[[max_index]]
```

Now we want to visualize this model, and there are several ways. First lets just look at the most probable words in topics. The topics argument should be limited to 5 topics so we can see all the words well. 

```{r}
# This will plot top 3 words and size of each topic, can also limit topics by using the topics argument in the plot function, ie plot(..., topics = c(1:5))
plot(opt_model32)

# If you wanna see more than three words, use this, adjusting topics limit as needed
plot(opt_model32, type = 'labels', topics = c(1:5))

# This creates a datatable of the top 20 words per topic
dt32topics_optmodel  = as.data.table(labelTopics(opt_model32, n = 20)$prob)

# This creates a datatable of the topic distribution over topic
dt32docdist_optmodel = make.dt(opt_model32, meta = out_3_40_stemmed$meta)

# And this opens an interactive visualizer for visuaizing the topics
toLDAvis(opt_model32, out_3_40_stemmed)
```
